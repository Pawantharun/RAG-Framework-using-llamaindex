{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Parallel File Processing for 3GPP Documents\n",
    "\n",
    "This Python script is designed to automate and optimize the processing of document files, specifically for a collection of 3GPP documents. It follows a two-step parallel processing approach to efficiently handle large volumes of files. The script operates within a specified base directory, targeting the `3GPP-all` folder, and processes documents found in its subdirectories.\n",
    "\n",
    "Key Features:\n",
    "1. `file_exists`: Verifies the existence of files, ensuring efficient handling of file operations.\n",
    "2. `unzip_task_directory`: Automates the unzipping of archives in the `3GPP-all` directory, with checks to avoid unnecessary processing of already unzipped files.\n",
    "3. Systematic traversal through nested directory structures, identifying and preparing files for processing.\n",
    "4. Implements `ThreadPoolExecutor` for parallel processing, significantly enhancing the efficiency of unzipping and document conversion tasks.\n",
    "5. Innovative use of multiple LibreOffice instances for parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "def unzip_task_directory(directory):\n",
    "    print(f\"Unzipping files in {directory}...\")\n",
    "    for item in os.listdir(directory):\n",
    "        if item.endswith('.zip'):\n",
    "            file_path = os.path.join(directory, item)\n",
    "            if not file_exists(file_path.replace('.zip', '')):\n",
    "                try:\n",
    "                    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(directory)\n",
    "                    print(f\"  Unzipped {item}\")\n",
    "                except zipfile.BadZipFile:\n",
    "                    print(f\"  Warning: {file_path} is not a valid zip file and will be skipped.\")\n",
    "            else:\n",
    "                print(f\"  Skipping unzipping {item}, already exists.\")\n",
    "\n",
    "\n",
    "base_directory = \"./3GPP-all\"\n",
    "base_directory = os.path.abspath(base_directory)\n",
    "directories_to_process = []\n",
    "\n",
    "# Collect directories for unzipping\n",
    "for release_dir in os.listdir(base_directory):\n",
    "    release_path = os.path.join(base_directory, release_dir)\n",
    "    if os.path.isdir(release_path):\n",
    "        for version_dir in os.listdir(release_path):\n",
    "            version_path = os.path.join(release_path, version_dir)\n",
    "            if os.path.isdir(version_path):\n",
    "                directories_to_process.append(version_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First ThreadPoolExecutor for unzipping\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(unzip_task_directory, directories_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Libreoffice to convert doc files to docx and the to markdown in parallel\n",
    "\n",
    "THey key problem I had solved:\n",
    "I ended up going with an advice for starting many libreoffice instances in parallel. This works by adding a -env:UserInstallation=file:///tmp/... command line variable:\n",
    "\n",
    "libreoffice -env:UserInstallation=file:///tmp/delete_me_#{timestamp} \\\n",
    "            --headless \\\n",
    "            --convert-to pdf \\\n",
    "            --outdir /tmp \\\n",
    "            /path/to/my_file.doc\n",
    "\n",
    "The advice itself was spotted in a long discussion to an issue on GitHub called \"Parallel conversions and synchronization\".\n",
    "\n",
    "GPT4 break down:\n",
    "The response you found describes a method for running multiple instances of LibreOffice in parallel for file conversion tasks. This technique is particularly useful when you need to process a large number of documents simultaneously, which can be a common requirement in server-side applications or batch processing scripts.\n",
    "\n",
    "Hereâ€™s a breakdown of the response and how the method works:\n",
    "\n",
    "1. **Multiple LibreOffice Instances**: By default, LibreOffice is designed to run as a single instance. This can be a limitation when trying to convert multiple documents at the same time, as each conversion task would need to wait for the previous one to complete.\n",
    "\n",
    "2. **Using `-env:UserInstallation`**: The key to running multiple instances is the `-env:UserInstallation` command-line option. This option allows you to specify a unique user profile directory for each LibreOffice instance. By setting a different user profile directory for each instance (like using a unique `/tmp/delete_me_#{timestamp}` in the example), you essentially isolate these instances from each other.\n",
    "\n",
    "3. **How it Works**:\n",
    "   - `libreoffice`: The command to run LibreOffice.\n",
    "   - `-env:UserInstallation=file:///tmp/delete_me_#{timestamp}`: This sets a unique user profile directory. The `#{timestamp}` part is a placeholder and should be replaced with a unique identifier for each instance, such as a timestamp or a unique sequence number.\n",
    "   - `--headless`: This option runs LibreOffice without its GUI, which is necessary for server-side or command-line operations.\n",
    "   - `--convert-to pdf`: This instructs LibreOffice to convert the input document to a PDF. This can be changed to other formats as needed.\n",
    "   - `--outdir /tmp`: Specifies the output directory for the converted file.\n",
    "   - `/path/to/my_file.doc`: The path to the document that needs to be converted.\n",
    "\n",
    "4. **Benefits**:\n",
    "   - **Parallel Processing**: This approach allows for true parallel processing of document conversions, significantly reducing the time required to process multiple files.\n",
    "   - **Isolation of Instances**: Each instance operates independently, reducing the chances of conflicts or crashes affecting other instances.\n",
    "\n",
    "5. **Use Cases**: This method is particularly beneficial in scenarios where you have to convert a large batch of documents in a short amount of time, such as in web servers, document management systems, or batch processing scripts.\n",
    "\n",
    "6. **Cleanup**: Since this approach creates temporary user profiles, it's important to implement a cleanup mechanism to delete these temporary directories after the conversions are complete to avoid cluttering the file system.\n",
    "\n",
    "This method is an effective solution for overcoming the limitations of LibreOffice's default single-instance mode, enabling efficient parallel processing of document conversion tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "\n",
    "def convert_doc_to_docx_and_markdown(doc_path):\n",
    "    directory = os.path.dirname(doc_path)\n",
    "    docx_path = doc_path + 'x'\n",
    "    markdown_file = os.path.splitext(docx_path)[0] + '.md'\n",
    "\n",
    "    # Ensure a temp directory exists in the current working directory\n",
    "    temp_dir = os.path.join(os.getcwd(), 'temp')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Generate a unique identifier for the LibreOffice user profile\n",
    "    unique_id = str(time.time()).replace('.', '')\n",
    "\n",
    "    # Create a LibreOffice user profile directory inside the temp folder\n",
    "    temp_libreoffice_dir = os.path.join(temp_dir, f\"libreoffice_temp_{unique_id}\")\n",
    "    os.makedirs(temp_libreoffice_dir, exist_ok=True)\n",
    "    user_installation_path = f\"file://{temp_libreoffice_dir}\"\n",
    "\n",
    "    # Convert DOC to DOCX\n",
    "    if not file_exists(docx_path):\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                \"libreoffice\", \n",
    "                \"-env:UserInstallation=\" + user_installation_path,\n",
    "                \"--headless\", \n",
    "                \"--convert-to\", \"docx\", \n",
    "                doc_path, \n",
    "                \"--outdir\", directory], \n",
    "                check=True, \n",
    "                stderr=subprocess.PIPE)\n",
    "            print(f\"    Converted {os.path.basename(doc_path)} to .docx\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"    Error converting {os.path.basename(doc_path)} to .docx: {e}\")\n",
    "            print(f\"    LibreOffice error: {e.stderr.decode()}\")\n",
    "\n",
    "    # Check if DOCX file exists before converting to Markdown\n",
    "    if file_exists(docx_path):\n",
    "        if not file_exists(markdown_file):\n",
    "            try:\n",
    "                subprocess.run(['pandoc', '-s', docx_path, '-o', markdown_file], check=True)\n",
    "                print(f\"    Converted {os.path.basename(docx_path)} to Markdown\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"    Error converting {os.path.basename(docx_path)} to Markdown: {e}\")\n",
    "    else:\n",
    "        print(f\"    {docx_path} does not exist. Skipping Markdown conversion.\")\n",
    "\n",
    "def process_task_file(doc_file):\n",
    "    print(f\"Processing {doc_file}...\")\n",
    "    convert_doc_to_docx_and_markdown(doc_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert files to docs and markdown format in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect .doc files for processing\n",
    "doc_files_to_process = []\n",
    "for directory in directories_to_process:\n",
    "    for item in os.listdir(directory):\n",
    "        if item.endswith('.doc'):\n",
    "            doc_files_to_process.append(os.path.join(directory, item))\n",
    "            \n",
    "# Second ThreadPoolExecutor for processing .doc files\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    list(executor.map(process_task_file, doc_files_to_process)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's clean up the folder. First we copy the files to a new folder and then keep only the markdown files and docx files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def clean_directory(directory, keep_extensions=['.docx', '.md']):\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(directory)])\n",
    "    processed_files = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            processed_files += 1\n",
    "            if not any(file.endswith(ext) for ext in keep_extensions):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Deleting: {file_path}\")\n",
    "                os.remove(file_path)  # Remove the file\n",
    "            \n",
    "            # Update and display the progress\n",
    "            progress = (processed_files / total_files) * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({processed_files}/{total_files})\")\n",
    "\n",
    "# Path to the directory you want to clean\n",
    "directory_path = './3GPP-clean'\n",
    "\n",
    "# Perform the cleaning\n",
    "clean_directory(directory_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3GPP-Clean Directory Markdown and DOCX File Size Analysis\n",
    "\n",
    "This Python script is designed to analyze the file sizes of Markdown (`.md`) documents in the `3GPP-clean` directory structure. The script will:\n",
    "\n",
    "1. Traverse through the `Rel-*` folders, each corresponding to a different release of the 3GPP documentation.\n",
    "2. Within each release, iterate through version subfolders.\n",
    "3. Calculate the accumulated file size of all `.md` files within each version and release.\n",
    "4. Compile this data into a comprehensive report, breaking down the sizes by version and release.\n",
    "5. Convert file sizes to a more human-readable format (megabytes).\n",
    "6. Save this report as a JSON file for easy reference.\n",
    "7. Print a summary to the console for the entire repository and each individual release.\n",
    "\n",
    "This utility is particularly useful for managing and understanding the distribution of document sizes within structured documentation repositories.\n",
    "\n",
    "### How to Run the Script\n",
    "\n",
    "- Ensure the script is executed in an environment with access to the `3GPP-clean` directory.\n",
    "- Modify `directory_path` in the script to point to the location of your `3GPP-clean` directory.\n",
    "- Run the script using a Python interpreter.\n",
    "- The output will be a JSON file named `md_sizes_report.json`, and a console printout of the summarized data.\n",
    "\n",
    "Below is the Python script that performs this analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of .md files in the repository: 4238.73 MB\n",
      "Rel-11: Total size of .md files: 256.03 MB\n",
      "Rel-10: Total size of .md files: 215.36 MB\n",
      "Rel-12: Total size of .md files: 280.84 MB\n",
      ".ipynb_checkpoints: Total size of .md files: 0.00 MB\n",
      "Rel-18: Total size of .md files: 594.03 MB\n",
      "Rel-14: Total size of .md files: 340.49 MB\n",
      "Rel-19: Total size of .md files: 9.28 MB\n",
      "Rel-15: Total size of .md files: 523.03 MB\n",
      "Rel-13: Total size of .md files: 353.18 MB\n",
      "Rel-16: Total size of .md files: 548.83 MB\n",
      "Rel-8: Total size of .md files: 186.79 MB\n",
      "Rel-9: Total size of .md files: 194.27 MB\n",
      "Rel-17: Total size of .md files: 736.61 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def bytes_to_megabytes(bytes_value):\n",
    "    return bytes_value / (1024 * 1024)\n",
    "\n",
    "def calculate_md_sizes(directory):\n",
    "    report = {\"total_size\": 0, \"releases\": {}}\n",
    "\n",
    "    for release in os.listdir(directory):\n",
    "        release_path = os.path.join(directory, release)\n",
    "        if os.path.isdir(release_path):\n",
    "            release_size = 0\n",
    "            report[\"releases\"][release] = {\"total_size\": 0, \"versions\": {}}\n",
    "\n",
    "            for version in os.listdir(release_path):\n",
    "                version_path = os.path.join(release_path, version)\n",
    "                if os.path.isdir(version_path):\n",
    "                    version_size = 0\n",
    "\n",
    "                    for file in os.listdir(version_path):\n",
    "                        if file.endswith('.md'):\n",
    "                            file_path = os.path.join(version_path, file)\n",
    "                            version_size += os.path.getsize(file_path)\n",
    "\n",
    "                    report[\"releases\"][release][\"versions\"][version] = bytes_to_megabytes(version_size)\n",
    "                    report[\"releases\"][release][\"total_size\"] += version_size\n",
    "                    release_size += version_size\n",
    "\n",
    "            report[\"releases\"][release][\"total_size\"] = bytes_to_megabytes(release_size)\n",
    "            report[\"total_size\"] += release_size\n",
    "\n",
    "    report[\"total_size\"] = bytes_to_megabytes(report[\"total_size\"])\n",
    "    return report\n",
    "\n",
    "def save_report_to_json(report, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(report, file, indent=4)\n",
    "\n",
    "def print_summary(report):\n",
    "    print(f\"Total size of .md files in the repository: {report['total_size']:.2f} MB\")\n",
    "    for release, data in report['releases'].items():\n",
    "        print(f\"{release}: Total size of .md files: {data['total_size']:.2f} MB\")\n",
    "\n",
    "# Main execution\n",
    "directory_path = './3GPP-clean'\n",
    "md_sizes_report = calculate_md_sizes(directory_path)\n",
    "json_filename = 'md_sizes_report.json'\n",
    "save_report_to_json(md_sizes_report, json_filename)\n",
    "print_summary(md_sizes_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3GPP Documentation Analysis\n",
    "\n",
    "This repository contains analysis data for the 3GPP documentation releases. The primary focus is on the file sizes of Markdown documents within each release.\n",
    "\n",
    "## File Size Analysis\n",
    "\n",
    "The analysis involves calculating the total size of Markdown (`.md`) files in each release of the 3GPP documentation. The data provides insights into the volume of documentation across different releases.\n",
    "\n",
    "### Graphical Representation\n",
    "\n",
    "Below is a bar plot that shows the total size of `.md` files in each release, from `Rel-8` to `Rel-19`. The sizes are represented in megabytes (MB).\n",
    "\n",
    "<!-- ![3GPP Releases MD File Sizes](results/3gpp_releases_md_file_sizes.png) -->\n",
    "<img src=\"3gpp_releases_md_file_sizes.png\" alt=\"3GPP Releases MD File Sizes\" width=\"50%\" height=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of .md files in the repository: 4238.73 MB\n",
      "Total words in .md files in the repository: 534914482\n",
      "Rel-11: Total size of .md files: 256.03 MB, Total words: 32817026\n",
      "Rel-10: Total size of .md files: 215.36 MB, Total words: 27820131\n",
      "Rel-12: Total size of .md files: 280.84 MB, Total words: 36218498\n",
      "Rel-18: Total size of .md files: 594.03 MB, Total words: 73825439\n",
      "Rel-14: Total size of .md files: 340.49 MB, Total words: 43484442\n",
      "Rel-19: Total size of .md files: 9.28 MB, Total words: 1221658\n",
      "Rel-15: Total size of .md files: 523.03 MB, Total words: 65165959\n",
      "Rel-13: Total size of .md files: 353.18 MB, Total words: 45118710\n",
      "Rel-16: Total size of .md files: 548.83 MB, Total words: 69425169\n",
      "Rel-8: Total size of .md files: 186.79 MB, Total words: 24117232\n",
      "Rel-9: Total size of .md files: 194.27 MB, Total words: 24953249\n",
      "Rel-17: Total size of .md files: 736.61 MB, Total words: 90746969\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def bytes_to_megabytes(bytes_value):\n",
    "    return bytes_value / (1024 * 1024)\n",
    "\n",
    "def count_words_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        contents = file.read()\n",
    "        words = contents.split()\n",
    "        return len(words)\n",
    "\n",
    "def calculate_md_sizes_and_word_count(directory):\n",
    "    report = {\"total_size\": 0, \"total_words\": 0, \"releases\": {}}\n",
    "\n",
    "    for release in os.listdir(directory):\n",
    "        release_path = os.path.join(directory, release)\n",
    "        if os.path.isdir(release_path):\n",
    "            release_size, release_word_count = 0, 0\n",
    "            report[\"releases\"][release] = {\"total_size\": 0, \"total_words\": 0, \"versions\": {}}\n",
    "\n",
    "            for version in os.listdir(release_path):\n",
    "                version_path = os.path.join(release_path, version)\n",
    "                if os.path.isdir(version_path):\n",
    "                    version_size, version_word_count = 0, 0\n",
    "\n",
    "                    for file in os.listdir(version_path):\n",
    "                        if file.endswith('.md'):\n",
    "                            file_path = os.path.join(version_path, file)\n",
    "                            version_size += os.path.getsize(file_path)\n",
    "                            version_word_count += count_words_in_file(file_path)\n",
    "\n",
    "                    report[\"releases\"][release][\"versions\"][version] = {\n",
    "                        \"size_mb\": bytes_to_megabytes(version_size),\n",
    "                        \"words\": version_word_count\n",
    "                    }\n",
    "                    report[\"releases\"][release][\"total_size\"] += version_size\n",
    "                    report[\"releases\"][release][\"total_words\"] += version_word_count\n",
    "                    release_size += version_size\n",
    "                    release_word_count += version_word_count\n",
    "\n",
    "            report[\"releases\"][release][\"total_size\"] = bytes_to_megabytes(release_size)\n",
    "            report[\"releases\"][release][\"total_words\"] = release_word_count\n",
    "            report[\"total_size\"] += release_size\n",
    "            report[\"total_words\"] += release_word_count\n",
    "\n",
    "    report[\"total_size\"] = bytes_to_megabytes(report[\"total_size\"])\n",
    "    return report\n",
    "\n",
    "def save_report_to_json(report, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(report, file, indent=4)\n",
    "\n",
    "def print_summary(report):\n",
    "    print(f\"Total size of .md files in the repository: {report['total_size']:.2f} MB\")\n",
    "    print(f\"Total words in .md files in the repository: {report['total_words']}\")\n",
    "    for release, data in report['releases'].items():\n",
    "        print(f\"{release}: Total size of .md files: {data['total_size']:.2f} MB, Total words: {data['total_words']}\")\n",
    "\n",
    "# Main execution\n",
    "directory_path = './3GPP-clean'\n",
    "md_sizes_report = calculate_md_sizes_and_word_count(directory_path)\n",
    "json_filename = 'md_sizes_word_count_report.json'\n",
    "save_report_to_json(md_sizes_report, json_filename)\n",
    "print_summary(md_sizes_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
